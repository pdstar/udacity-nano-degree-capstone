{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Modeling for US Immigration Department\n",
    "\n",
    "## Introduction: US immigration Department and Their Analytical Goals\n",
    "\n",
    "The US immigration Department has grown their immigration record database and wants to move their processes and analytical data onto the cloud. \n",
    "\n",
    "This project builds  an ETL pipeline that extracts US immigration Department data and other supporting data from variouys sources. This includes transforming data using Spark into a set of dimension and fact tables and loading it nto S3 for their analytics team to explore and find insights about US immigrants. \n",
    "\n",
    "Few key objectives of generating insight is as follows:\n",
    "- enable more efficient and immigrant friendly\n",
    "- identify process gaps and security gaps\n",
    "- forecast resource capacity required by immigration department\n",
    "- report important reports to various Federal bodies\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project: in README Section\n",
    "* Step 2: Extract: Gathering & Reading Raw Data from various sources\n",
    "* Step 3: Explore and Assess the Data\n",
    "* Step 4: Transform: Building the Data Model, creating Dimension & Fact Tables for the Schema\n",
    "* Step 5: Data Quality Checks\n",
    "* Step 6: Load Data into DestinationÂ¶\n",
    "* Step 7: Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import asc\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "from pyspark.sql.functions import monotonically_increasing_id,row_number\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def set_df_columns_nullable(spark, df, column_list, nullable=True):\n",
    "    for struct_field in df.schema:\n",
    "        if struct_field.name in column_list:\n",
    "            struct_field.nullable = nullable\n",
    "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
    "    return df_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope of Project: Details in *README Section*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Extract: Gathering & Reading Raw Data from various sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading immigration data - Apr 2016 data from source within Workspace\n",
    "#fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "#df_img = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#instantiating Spark Session\n",
    "#spark = SparkSession.builder.\\\n",
    "#config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "#config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "#enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading immigration data - Apr 2016 data from source within Spark \n",
    "# no need to run as data is already present\n",
    "#processing the entire Immigration file for Apr 2016\n",
    "#df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "#df_spark.printSchema()\n",
    "# write to parquet \n",
    "# df_spark.write.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading immigration data - Apr 2016 data from source in Spark \n",
    "I94_df=spark.read.parquet(\"input_data/sas_data\") \n",
    "I94_df.printSchema()\n",
    "I94_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# UDF to Converting SAS date to datetime\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types as T\n",
    "def convert_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "udf_datetime_from_sas = udf(lambda x: convert_datetime(x), T.DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----------+----------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|  arr_date|  dep_date|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----------+----------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|2016-04-30|2016-05-08|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+----------+----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "I94_df = I94_df.withColumn(\"arr_date\", udf_datetime_from_sas(I94_df.arrdate))\n",
    "I94_df = I94_df.withColumn(\"dep_date\", udf_datetime_from_sas(I94_df.depdate))\n",
    "\n",
    "I94_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>Long Beach</td>\n",
       "      <td>California</td>\n",
       "      <td>34.6</td>\n",
       "      <td>238159.0</td>\n",
       "      <td>236013.0</td>\n",
       "      <td>474172</td>\n",
       "      <td>17463.0</td>\n",
       "      <td>127764.0</td>\n",
       "      <td>2.78</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>64948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.2</td>\n",
       "      <td>65896.0</td>\n",
       "      <td>63116.0</td>\n",
       "      <td>129012</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>63413.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>NJ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>29822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>Huntsville</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.1</td>\n",
       "      <td>91764.0</td>\n",
       "      <td>97350.0</td>\n",
       "      <td>189114</td>\n",
       "      <td>16637.0</td>\n",
       "      <td>12691.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>6566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            City       State  Median Age  Male Population  Female Population  \\\n",
       "1683  Long Beach  California        34.6         238159.0           236013.0   \n",
       "221    Elizabeth  New Jersey        34.2          65896.0            63116.0   \n",
       "1245  Huntsville     Alabama        38.1          91764.0            97350.0   \n",
       "\n",
       "      Total Population  Number of Veterans  Foreign-born  \\\n",
       "1683            474172             17463.0      127764.0   \n",
       "221             129012              1131.0       63413.0   \n",
       "1245            189114             16637.0       12691.0   \n",
       "\n",
       "      Average Household Size State Code                       Race  Count  \n",
       "1683                    2.78         CA  Black or African-American  64948  \n",
       "221                     3.18         NJ  Black or African-American  29822  \n",
       "1245                    2.18         AL                      Asian   6566  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading US Demographics data\n",
    "fname = 'input_data/us-cities-demographics.csv'\n",
    "df_usdemog_t = pd.read_csv(fname, delimiter =\";\")\n",
    "df_usdemog_t.sample(3)\n",
    "#len(df_usdemog.index) #2891\n",
    "#df_usdemog.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City              object\n",
       "State             object\n",
       "median_age       float64\n",
       "male_pop           int64\n",
       "female_pop         int64\n",
       "tot_pop            int64\n",
       "veteran_pop        int64\n",
       "foreign_born       int64\n",
       "avg_hose_size      int64\n",
       "state_code        object\n",
       "Race              object\n",
       "Count              int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_usdemog = df_usdemog.astype({\"Male Population\":\"int\",\"Female Population\":\"int\"})\n",
    "float_to_int = [\"Male Population\", \"Female Population\", \"Number of Veterans\", \"Foreign-born\", \"Average Household Size\" ]\n",
    "df_usdemog_t[float_to_int] = df_usdemog_t[float_to_int].fillna(0).astype(int)\n",
    "\n",
    "rename_col={'Median Age': 'median_age', 'Male Population': 'male_pop', 'Female Population': 'female_pop', \n",
    "            'Total Population': 'tot_pop', 'Number of Veterans': 'veteran_pop', 'Foreign-born': 'foreign_born', \n",
    "            'Average Household Size': 'avg_hose_size', 'State Code': 'state_code'}\n",
    "df_usdemog = df_usdemog_t.rename(rename_col, axis='columns')\n",
    "df_usdemog.dtypes\n",
    "#df_usdemog.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reading Temperature Data\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = pd.read_csv(fname)\n",
    "df_temp['Country'] = df_temp['Country'].str.upper()\n",
    "df_temp['dt'] = pd.to_datetime(df_temp['dt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2582706</th>\n",
       "      <td>2004-11-01</td>\n",
       "      <td>20.032</td>\n",
       "      <td>0.194</td>\n",
       "      <td>Gizeh</td>\n",
       "      <td>EGYPT</td>\n",
       "      <td>29.74N</td>\n",
       "      <td>31.38E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5853245</th>\n",
       "      <td>1943-11-01</td>\n",
       "      <td>25.752</td>\n",
       "      <td>0.190</td>\n",
       "      <td>Phnum PÃ©nh</td>\n",
       "      <td>CAMBODIA</td>\n",
       "      <td>12.05N</td>\n",
       "      <td>105.21E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907795</th>\n",
       "      <td>1812-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bhiwandi</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>20.09N</td>\n",
       "      <td>73.36E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6140490</th>\n",
       "      <td>1974-06-01</td>\n",
       "      <td>19.336</td>\n",
       "      <td>0.525</td>\n",
       "      <td>Qingdao</td>\n",
       "      <td>CHINA</td>\n",
       "      <td>36.17N</td>\n",
       "      <td>121.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3366331</th>\n",
       "      <td>1846-05-01</td>\n",
       "      <td>20.444</td>\n",
       "      <td>2.074</td>\n",
       "      <td>Jammu</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>75.64E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "2582706 2004-11-01              20.032                          0.194   \n",
       "5853245 1943-11-01              25.752                          0.190   \n",
       "907795  1812-08-01                 NaN                            NaN   \n",
       "6140490 1974-06-01              19.336                          0.525   \n",
       "3366331 1846-05-01              20.444                          2.074   \n",
       "\n",
       "               City   Country Latitude Longitude  \n",
       "2582706       Gizeh     EGYPT   29.74N    31.38E  \n",
       "5853245  Phnum PÃ©nh  CAMBODIA   12.05N   105.21E  \n",
       "907795     Bhiwandi     INDIA   20.09N    73.36E  \n",
       "6140490     Qingdao     CHINA   36.17N   121.33E  \n",
       "3366331       Jammu     INDIA   32.95N    75.64E  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_temp.dtypes\n",
    "#len(df_temp.index) #8599212\n",
    "df_temp.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Mapping tables\n",
    "\n",
    "# Reading country codes data \n",
    "fname = 'input_data/country_mapping.csv'\n",
    "dim_countrymapping = pd.read_csv(fname)\n",
    "\n",
    "# Reading US state codes data \n",
    "fname = 'input_data/state_code_dim.csv'\n",
    "dim_us_state = pd.read_csv(fname)\n",
    "\n",
    "#Reading port codes\n",
    "fname = 'input_data/port_code.csv'\n",
    "dim_port = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Explore and Assess the Data\n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### *Exploring Immigration data*\n",
    "\n",
    "- 3096313 records\n",
    "- Several columns have null values: i94mode, 94addr, depdate, i94bir, visapost,occup, entdepa, entdepd, entdepu, matflag, biryear, dtaddto, gender, insnum, airline, fltno, dep_date\n",
    "- Yougest member was born in 2019. But that is incorrect as the its a Apr 2016 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Null entires in raw immigration data \n",
    "I94_df_null = I94_df.select([count(when(col(c).isNull(), c)).alias(c) for c in I94_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#I94_df_null = I94_df_null.toPandas()\n",
    "I94_df_null.to_csv('I94_df_null_f.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Summary of columns in raw immigration data; statistics: count, mean, stddev, min, max \n",
    "I94_df_summary = I94_df.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#I94_df_summary = I94_df_summary.toPandas()\n",
    "I94_df_summary.to_csv('I94_df_summary.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### *Exploring US demographic data*\n",
    "- 2891 records\n",
    "- All fields have 100% fill rates\n",
    "- There are no duplicates\n",
    "- Average household size is 2.3 which looks a bit low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null entries :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "City                      0\n",
       "State                     0\n",
       "Median Age                0\n",
       "Male Population           0\n",
       "Female Population         0\n",
       "Total Population          0\n",
       "Number of Veterans        0\n",
       "Foreign-born              0\n",
       "Average Household Size    0\n",
       "State Code                0\n",
       "Race                      0\n",
       "Count                     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding Null entires in raw US demographic data \n",
    "print(\"Null entries :\")\n",
    "df_usdemog_t.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [City, State, Median Age, Male Population, Female Population, Total Population, Number of Veterans, Foreign-born, Average Household Size, State Code, Race, Count]\n",
       "Index: []"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finding duplicate records in raw US demographic data \n",
    "duplicate_usdemog = df_usdemog_t[df_usdemog_t.duplicated()]\n",
    "print(\"Duplicate Rows :\")\n",
    "duplicate_usdemog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2891.000000</td>\n",
       "      <td>2.891000e+03</td>\n",
       "      <td>2.891000e+03</td>\n",
       "      <td>2.891000e+03</td>\n",
       "      <td>2891.000000</td>\n",
       "      <td>2.891000e+03</td>\n",
       "      <td>2891.000000</td>\n",
       "      <td>2.891000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.494881</td>\n",
       "      <td>9.722743e+04</td>\n",
       "      <td>1.016640e+05</td>\n",
       "      <td>1.989668e+05</td>\n",
       "      <td>9325.708059</td>\n",
       "      <td>4.047079e+04</td>\n",
       "      <td>2.223798</td>\n",
       "      <td>4.896377e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.401617</td>\n",
       "      <td>2.162104e+05</td>\n",
       "      <td>2.314676e+05</td>\n",
       "      <td>4.475559e+05</td>\n",
       "      <td>13196.370589</td>\n",
       "      <td>1.554222e+05</td>\n",
       "      <td>0.485144</td>\n",
       "      <td>1.443856e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>22.900000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>6.321500e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.800000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.800000</td>\n",
       "      <td>3.928900e+04</td>\n",
       "      <td>4.121250e+04</td>\n",
       "      <td>8.042900e+04</td>\n",
       "      <td>3728.500000</td>\n",
       "      <td>9.084000e+03</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.435000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.300000</td>\n",
       "      <td>5.233600e+04</td>\n",
       "      <td>5.380900e+04</td>\n",
       "      <td>1.067820e+05</td>\n",
       "      <td>5394.000000</td>\n",
       "      <td>1.866600e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.378000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.659600e+04</td>\n",
       "      <td>8.958900e+04</td>\n",
       "      <td>1.752320e+05</td>\n",
       "      <td>9367.500000</td>\n",
       "      <td>3.387800e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.444700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.500000</td>\n",
       "      <td>4.081698e+06</td>\n",
       "      <td>4.468707e+06</td>\n",
       "      <td>8.550405e+06</td>\n",
       "      <td>156961.000000</td>\n",
       "      <td>3.212500e+06</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.835726e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Median Age  Male Population  Female Population  Total Population  \\\n",
       "count  2891.000000     2.891000e+03       2.891000e+03      2.891000e+03   \n",
       "mean     35.494881     9.722743e+04       1.016640e+05      1.989668e+05   \n",
       "std       4.401617     2.162104e+05       2.314676e+05      4.475559e+05   \n",
       "min      22.900000     0.000000e+00       0.000000e+00      6.321500e+04   \n",
       "25%      32.800000     3.928900e+04       4.121250e+04      8.042900e+04   \n",
       "50%      35.300000     5.233600e+04       5.380900e+04      1.067820e+05   \n",
       "75%      38.000000     8.659600e+04       8.958900e+04      1.752320e+05   \n",
       "max      70.500000     4.081698e+06       4.468707e+06      8.550405e+06   \n",
       "\n",
       "       Number of Veterans  Foreign-born  Average Household Size         Count  \n",
       "count         2891.000000  2.891000e+03             2891.000000  2.891000e+03  \n",
       "mean          9325.708059  4.047079e+04                2.223798  4.896377e+04  \n",
       "std          13196.370589  1.554222e+05                0.485144  1.443856e+05  \n",
       "min              0.000000  0.000000e+00                0.000000  9.800000e+01  \n",
       "25%           3728.500000  9.084000e+03                2.000000  3.435000e+03  \n",
       "50%           5394.000000  1.866600e+04                2.000000  1.378000e+04  \n",
       "75%           9367.500000  3.387800e+04                2.000000  5.444700e+04  \n",
       "max         156961.000000  3.212500e+06                4.000000  3.835726e+06  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#description of raw US demographic data \n",
    "print(\"Description :\")\n",
    "df_usdemog_t.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### *Exploring Temperature Data*\n",
    "- 8235082 rows\n",
    "- 364130 have null AverageTemperature and AverageTemperatureUncertainty\n",
    "- All other field have 100% fill rate\n",
    "- There are no duplicate rows\n",
    "- Average temeperatir is 18 Degrees, with max being close to 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null entries :\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dt                                    0\n",
       "AverageTemperature               364130\n",
       "AverageTemperatureUncertainty    364130\n",
       "City                                  0\n",
       "Country                               0\n",
       "Latitude                              0\n",
       "Longitude                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Null entires in raw Temperature Data \n",
    "print(\"Null entries :\")\n",
    "df_temp.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate Rows :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Findng duplicate records in raw US demographic data \n",
    "duplicate_df_temp = df_temp[df_temp.duplicated()]\n",
    "print(\"Duplicate Rows :\")\n",
    "duplicate_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.235082e+06</td>\n",
       "      <td>8.235082e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.672743e+01</td>\n",
       "      <td>1.028575e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.035344e+01</td>\n",
       "      <td>1.129733e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.270400e+01</td>\n",
       "      <td>3.400000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.029900e+01</td>\n",
       "      <td>3.370000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.883100e+01</td>\n",
       "      <td>5.910000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.521000e+01</td>\n",
       "      <td>1.349000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.965100e+01</td>\n",
       "      <td>1.539600e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AverageTemperature  AverageTemperatureUncertainty\n",
       "count        8.235082e+06                   8.235082e+06\n",
       "mean         1.672743e+01                   1.028575e+00\n",
       "std          1.035344e+01                   1.129733e+00\n",
       "min         -4.270400e+01                   3.400000e-02\n",
       "25%          1.029900e+01                   3.370000e-01\n",
       "50%          1.883100e+01                   5.910000e-01\n",
       "75%          2.521000e+01                   1.349000e+00\n",
       "max          3.965100e+01                   1.539600e+01"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#description of raw Temperature data \n",
    "df_temp.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Transform: Building the Data Model, creating Dimension & Fact Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension Table: US_Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Dimension table: US_Demographics\n",
    "\n",
    "dim_usdemog = df_usdemog.drop(columns=['State'])\n",
    "dim_usdemog.insert(loc=0, column='demog_id', value=(dim_usdemog.index+1))\n",
    "#dim_usdemog.sample(3)\n",
    "#dim_usdemog.dtypes\n",
    "dim_usdemog=spark.createDataFrame(dim_usdemog) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- demog_id: long (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_pop: long (nullable = true)\n",
      " |-- female_pop: long (nullable = true)\n",
      " |-- tot_pop: long (nullable = true)\n",
      " |-- veteran_pop: long (nullable = true)\n",
      " |-- foreign_born: long (nullable = true)\n",
      " |-- avg_hose_size: long (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_usdemog.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension Table: Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Dimension table: Calendar\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import types as T\n",
    "def convert_datetime(x):\n",
    "    try:\n",
    "        start = datetime(1960, 1, 1)\n",
    "        return start + timedelta(days=int(x))\n",
    "    except:\n",
    "        return None\n",
    "udf_datetime_from_sas = udf(lambda x: convert_datetime(x), T.DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "I94_df = I94_df.withColumn(\"arr_date\", udf_datetime_from_sas(I94_df.arrdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- arr_date: date (nullable = false)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# extract columns to create time table\n",
    "dim_calendar = I94_df.selectExpr(\n",
    "    \"arr_date as arr_date\",\n",
    "    \"dayofmonth(arr_date) as day\",\n",
    "    \"weekofyear(arr_date) as week\",\n",
    "    \"month(arr_date) as month\",\n",
    "    \"year(arr_date) as year\",\n",
    "    \"dayofweek(arr_date) as weekday\"\n",
    "    )\n",
    "    \n",
    "dim_calendar= dim_calendar.dropDuplicates()\n",
    "dim_calendar = set_df_columns_nullable(spark,dim_calendar,['arr_date'], False)\n",
    "\n",
    "dim_calendar.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_calendar.count()\n",
    "#dim_calendar.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension Table: Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3005913</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>25.783</td>\n",
       "      <td>0.431</td>\n",
       "      <td>Hoshangabad</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>23.31N</td>\n",
       "      <td>77.77E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250141</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>18.652</td>\n",
       "      <td>0.595</td>\n",
       "      <td>Americana</td>\n",
       "      <td>BRAZIL</td>\n",
       "      <td>23.31S</td>\n",
       "      <td>48.06W</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6600467</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>18.346</td>\n",
       "      <td>0.350</td>\n",
       "      <td>Salzgitter</td>\n",
       "      <td>GERMANY</td>\n",
       "      <td>52.24N</td>\n",
       "      <td>10.51E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1449387</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>26.980</td>\n",
       "      <td>0.369</td>\n",
       "      <td>Chandrapur</td>\n",
       "      <td>INDIA</td>\n",
       "      <td>20.09N</td>\n",
       "      <td>78.48E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6038754</th>\n",
       "      <td>2013-08-01</td>\n",
       "      <td>15.384</td>\n",
       "      <td>0.814</td>\n",
       "      <td>Pretoria</td>\n",
       "      <td>SOUTH AFRICA</td>\n",
       "      <td>24.92S</td>\n",
       "      <td>28.37E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                dt  AverageTemperature  AverageTemperatureUncertainty  \\\n",
       "3005913 2013-08-01              25.783                          0.431   \n",
       "250141  2013-08-01              18.652                          0.595   \n",
       "6600467 2013-08-01              18.346                          0.350   \n",
       "1449387 2013-08-01              26.980                          0.369   \n",
       "6038754 2013-08-01              15.384                          0.814   \n",
       "\n",
       "                City       Country Latitude Longitude  \n",
       "3005913  Hoshangabad         INDIA   23.31N    77.77E  \n",
       "250141     Americana        BRAZIL   23.31S    48.06W  \n",
       "6600467   Salzgitter       GERMANY   52.24N    10.51E  \n",
       "1449387   Chandrapur         INDIA   20.09N    78.48E  \n",
       "6038754     Pretoria  SOUTH AFRICA   24.92S    28.37E  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Dimension table: Weather\n",
    "dim_weather_t1 = df_temp.dropna(subset=['dt', 'AverageTemperature','City', 'Country'])\n",
    "dim_weather_t1 = dim_weather_t1.sort_values(['City', 'dt']).drop_duplicates('City', keep='last')\n",
    "#len(dim_weather_t1.index)\n",
    "dim_weather_t1.shape[0]\n",
    "dim_weather_t1.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weather_id                                int64\n",
       "country_code                              int64\n",
       "Country                                  object\n",
       "measure_date                     datetime64[ns]\n",
       "AverageTemperature                      float64\n",
       "AverageTemperatureUncertainty           float64\n",
       "City                                     object\n",
       "Latitude                                 object\n",
       "Longitude                                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Including country code which connects with Fact table\n",
    "dim_weather = pd.merge(dim_countrymapping, dim_weather_t1, on='Country', suffixes=('_map','_wthr'),how='right')\n",
    "len(dim_weather.index)\n",
    "dim_weather.insert(loc=0, column='weather_id', value=(dim_weather.index+1))\n",
    "dim_weather.rename(columns={'dt': 'measure_date'}, inplace=True)\n",
    "#dim_weather[['country_code']] = dim_weather[['country_code']].astype(int)\n",
    "dim_weather[['country_code']] = dim_weather[['country_code']].fillna(0).astype(int)\n",
    "dim_weather.sample(5)\n",
    "#dim_weather['Country'].unique()\n",
    "dim_weather.dtypes\n",
    "#len(dim_weather.index) #3448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_weather=spark.createDataFrame(dim_weather) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- weather_id: long (nullable = true)\n",
      " |-- country_code: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- measure_date: timestamp (nullable = true)\n",
      " |-- AverageTemperature: double (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: double (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dim_weather[dim_weather['country_code'].isna()]['Country'].unique()\n",
    "dim_weather.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Dimension Table: Weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- immig_id: integer (nullable = false)\n",
      " |-- country_origin: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insurance_num: string (nullable = true)\n",
      " |-- visa_category: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Dimension table: Immigrants\n",
    "dim_immgrant_t1 = I94_df.selectExpr(\n",
    "    \"int(cicid) as immig_id\",\n",
    "    \"int(I94CIT) as country_origin\",\n",
    "    \"int(BIRYEAR) as birth_year\",\n",
    "    \"GENDER as gender\",\n",
    "    \"INSNUM as insurance_num\",\n",
    "    \"CASE WHEN int(I94VISA) == 1 THEN  'Business' WHEN int(I94VISA) == 2 THEN  'Pleasure' WHEN int(I94VISA) == 3 THEN 'Student' ELSE 'other' END AS visa_category\"\n",
    "    )  \n",
    "dim_immgrant= dim_immgrant_t1.dropDuplicates(['immig_id'])\n",
    "dim_immgrant = set_df_columns_nullable(spark,dim_immgrant,['visa_category'])\n",
    "dim_immgrant = set_df_columns_nullable(spark,dim_immgrant,['immig_id'], False)\n",
    "#dim_immgrant = pd.merge(dim_countrymapping, dim_immgrant, on='Country', suffixes=('_map','_wthr'),how='right')\n",
    "\n",
    "\n",
    "#print(\"dim_immgrant_t1: \", dim_immgrant_t1.count())\n",
    "#print(\"dim_immgrant: \", dim_immgrant.count())\n",
    "dim_immgrant.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Fact Table: Immigration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+----------+----+-------+-------+----------+--------+-------+-------+---------------+-----+--------+--------+\n",
      "|cicid|I94CIT|I94PORT|  arr_date|year|I94MODE|I94ADDR|  dep_date| I94VISA|MATFLAG|AIRLINE|         ADMNUM|FLTNO|VISATYPE|immig_id|\n",
      "+-----+------+-------+----------+----+-------+-------+----------+--------+-------+-------+---------------+-----+--------+--------+\n",
      "| 8029|   111|    LOS|2016-04-01|2016|      1|   null|2016-04-02|Pleasure|      M|     AF|5.5461196133E10|00077|      WT|       1|\n",
      "|12272|   114|    MIA|2016-04-01|2016|      2|   null|2016-04-05|Pleasure|      M|    VES|5.5411217733E10|91285|      WT|       2|\n",
      "|13442|   116|    PBB|2016-04-01|2016|      3|   null|2016-04-02|Pleasure|      M|   null| 8.223882253E10| LAND|      B2|       3|\n",
      "+-----+------+-------+----------+----+-------+-------+----------+--------+-------+-------+---------------+-----+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fact table: Immigration\n",
    "fact_immigrant_t1 = I94_df.selectExpr(\n",
    "    \"int(cicid) as cicid\",\n",
    "    \"int(I94CIT) as I94CIT\",\n",
    "    \"I94PORT\",\n",
    "    \"arr_date\",\n",
    "    \"year(arr_date) as year\",\n",
    "    \"int(I94MODE) as I94MODE\",\n",
    "    \"I94ADDR\",\n",
    "    \"dep_date\",\n",
    "    \"CASE WHEN int(I94VISA) == 1 THEN  'Business' WHEN int(I94VISA) == 2 THEN  'Pleasure' WHEN int(I94VISA) == 3 THEN 'Student' ELSE 'other' END AS I94VISA\",\n",
    "    \"MATFLAG\",\n",
    "    \"AIRLINE\",\n",
    "    \"ADMNUM\",\n",
    "    \"FLTNO\",\n",
    "    \"VISATYPE\")  \n",
    "fact_immigrant = fact_immigrant_t1.dropDuplicates()\n",
    "fact_immigrant = set_df_columns_nullable(spark,fact_immigrant,['cicid'], False)\n",
    "fact_immigrant = set_df_columns_nullable(spark,fact_immigrant,['I94VISA'])\n",
    "#fact_immigrant = fact_immigrant.withColumn(\"immig_id\",row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "fact_immigrant = fact_immigrant.withColumn('immig_id', monotonically_increasing_id()+1)\n",
    "fact_immigrant.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = false)\n",
      " |-- I94CIT: integer (nullable = true)\n",
      " |-- I94PORT: string (nullable = true)\n",
      " |-- arr_date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- I94MODE: integer (nullable = true)\n",
      " |-- I94ADDR: string (nullable = true)\n",
      " |-- dep_date: date (nullable = true)\n",
      " |-- I94VISA: string (nullable = true)\n",
      " |-- MATFLAG: string (nullable = true)\n",
      " |-- AIRLINE: string (nullable = true)\n",
      " |-- ADMNUM: double (nullable = true)\n",
      " |-- FLTNO: string (nullable = true)\n",
      " |-- VISATYPE: string (nullable = true)\n",
      " |-- immig_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigrant.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Data Quality Checks\n",
    "2 quality checks are performed:\n",
    " * Count checks to ensure completeness: checks if the output schema tables are empty\n",
    " * Integrity constraints on the relational database: primary key should not be null in any schema table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks: empty table\n",
    "def empty_table(tables, tablename):\n",
    "    \"\"\"\n",
    "    Checks if the schema tables, dim and fact, are empty \n",
    "    Parameters: \n",
    "        tables: list of table objects\n",
    "        tablename: list of tale names\n",
    "    Output:\n",
    "        fail: flag (1 any table is empty)\n",
    "        fail_table: list of empty tables \n",
    "    \"\"\"\n",
    "    fail = 0\n",
    "    fail_table = []\n",
    "    i = 0\n",
    "    for table in tables:\n",
    "        count = table.count()\n",
    "        if count < 1:\n",
    "            fail = 1\n",
    "            fail_table.append(table)\n",
    "            print(f\"Data quality check failed for {tablename[i]}: 0 records\")\n",
    "        print(f\"Data quality check passed for {tablename[i]}: {count} records\")\n",
    "        i = i+1\n",
    "    return fail, fail_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for dim_usdemog: 2891 records\n",
      "Data quality check passed for dim_weather: 3448 records\n",
      "0 []\n"
     ]
    }
   ],
   "source": [
    "tables = [dim_usdemog, dim_weather]\n",
    "tablename = ['dim_usdemog', 'dim_weather']\n",
    "(fail_flag, failed_table) = empty_table(tables, tablename)\n",
    "print(fail_flag,failed_table )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks: null primary key table\n",
    "def null_key(tables, tablename, primary_key):\n",
    "    \"\"\"\n",
    "    Checks if the any primary key in schema tables, dim and fact, has null value \n",
    "    Parameters: \n",
    "        tables: list of table objects\n",
    "        tablename: list of tale names\n",
    "        primary_key: primary keys of the respective tables\n",
    "    Output:\n",
    "        fail: 1 any table is empty\n",
    "        fail_table: list of empty tables \n",
    "    \"\"\"\n",
    "    fail = 0\n",
    "    fail_table = []\n",
    "    i = 0\n",
    "    for table in tables:\n",
    "        count = table.where(F.col(primary_key[i]).isNull()).count()\n",
    "        if count > 1:\n",
    "            fail = 1\n",
    "            fail_table.append(table)\n",
    "            print(f\"Data quality check failed for {tablename[i]}: {count} records\")\n",
    "        print(f\"Data quality check passed for {tablename[i]}: {count} records\")\n",
    "        i = i+1\n",
    "    return fail, fail_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for dim_usdemog: 0 records\n",
      "Data quality check passed for dim_weather: 0 records\n",
      "0 []\n"
     ]
    }
   ],
   "source": [
    "tables = [dim_usdemog, dim_weather]\n",
    "tablename = ['dim_usdemog', 'dim_weather']\n",
    "primary_key = ['demog_id', 'weather_id']\n",
    "(fail_flag, failed_table) = null_key(tables, tablename, primary_key)\n",
    "print(fail_flag,failed_table )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 6: Load Data into Destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write tables table to parquet files\n",
    "output_data = \"output_data/\"\n",
    "dim_calendar.write.mode('overwrite').partitionBy(\"year\").parquet(output_data + \"dim_calendar\")\n",
    "dim_weather.write.mode('overwrite').parquet(output_data + \"dim_weather\")\n",
    "dim_usdemog.write.mode('overwrite').parquet(output_data + \"dim_usdemog\")\n",
    "fact_immigration.write.mode('overwrite').partitionBy(\"arr_year\").parquet(output_data + \"fact_immigration\")\n",
    "dim_immigrant.write.mode('overwrite').partitionBy(\"arr_year\").parquet(output_data + \"dim_immigrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 6: Project Write Up\n",
    "\n",
    "#### 6.1: Data Dictionary\n",
    "Can be found [here](Data_dictionary.csv) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 6.2 Data Model: Star Schema\n",
    "\n",
    "##### Why Use Star Schema\n",
    "With Star Schema there is no need for complex joins when querying data. This makes it very easy to use for business and BI teams. And as a results, queries also run faster as there are no elaborate joins. \n",
    "\n",
    "Its is also easy to understand once built and hence any modification is also simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "\n",
    "#### 6.3.Tool and Technology Used\n",
    "Spark (PySpark and Spark SQL), Python, and AWS S3 has been used to build this ETL pipeline. \n",
    "* Since the ETL is done on big data, Spark provides the computing power through it Distributed processing framework. \n",
    "* The objective of this initiative is to able host the OLAP DB on the Cloud to enable improved accessibility, security and computing prower in downstream analytics workstream. AWS S3 enable this objective\n",
    "* Python is the Easy to Use, Highly Compatible with other framworkds such as Spark and AWS, has lots of powerful libraries, and built in data stuctures which makes a labguage of choice for data engineering.\n",
    "\n",
    "#### 6.4 Approach to problem under different scenarios:\n",
    "**The data was increased by 100x**: \n",
    "If the data is incresed by 100x, we will need more compute/processing power. This can be done by using using AWS EMR clusters. EMR simplifies running big data frameworks such as to process and analyze vast amounts of data.\n",
    "\n",
    "**The data populates a dashboard that must be updated on a daily basis by 7am every day**: \n",
    "Airflow can be used for automated orchestration of data pipeline as in this case. Airflow DAG with appropriate start/end criteria will enable this\n",
    "\n",
    "**The database needed to be accessed by 100+ people**: \n",
    "We can shift to AWS Redshift to query as it provides high availability and limitless concurrency. This is also cost efficient as Redshift doesn't charge per query but by total query volume."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
